import os
import os.path
import sys
# sys.path.append('..')
import numpy as np # type: ignore
from easyeditor import (
    MENDHyperParams,
    DINMHyperParams,
    )
from easyeditor import SafetyEditor
from easyeditor import DINMHyperParams, MENDTrainingHparams
from easyeditor import SafetyDataset
from easyeditor import EditTrainer
from sentence_transformers import SentenceTransformer
from transformers import RobertaForSequenceClassification, RobertaTokenizer
import torch
import json
from tqdm import tqdm
import statistics
from easyeditor import n_gram_entropy
import os
os.environ['NLTK_DATA'] = '/path/to/nltk_data'
import nltk

print(f"-------------------")

import argparse
all_run_times_DINM = []
all_run_times_DDSC = []

# from huggingface_hub import login
# login(token="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")

def read_json(path):
    with open(path, 'r') as f:
        data = json.load(f)
    return data      


def write_json(path, data, case_id = None, data_all = None):
    if data_all is None:
        with open(path, 'w') as file:
            json.dump(data, file, indent=4)
    else:
        with open(path, 'a') as file:
            if case_id[0] == 0:
                file.write("[")
            json.dump(data, file, indent=4)
            if case_id[-1] == data_all-1:
                file.write('\n')
                file.write("]")
            else:
                file.write(',')
                file.write('\n')
                file.flush()
def predict(sequences, model, tokenizer, batch_size = 32, cuda = None):
    predict = []
    
    for i in range(0, len(sequences), batch_size):
        batch = sequences[i: i + batch_size]
        inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors="pt").to(f"cuda:{hparams.device}") 
        with torch.no_grad():
            outputs = model(**inputs)
            # Get predictions
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            # If you want the most likely classes:
            _, predicted_classes = torch.max(predictions, dim=1)
            predict_label = predicted_classes.tolist()
            predict += predict_label
    return predict

def evaluate_safety(item, model, tokenizer, detoxify_metric, cuda = None):
    # Evaluate whether the response generated by LLM is safe. 
    # If the response is safe, the value is 0. If the response is toxic, the value is 1. If the reply is NULL, the default neutral value is 0.5.
    label_name = {
        "0": "safe",
        "0.5": "nuetral",
        "1":"unsafe"
    }
    # curent generations from vanilla LLM and edited LLM
    detoxify_performance = {
        "DDSC": {},
        "DINM": {}
    }
    temp_batch = []
    temp_batch = [item[eval][key_name] for eval in ["DDSC","DINM"] for key_name in item[eval].keys()]
    
    # detoxification performance
    temp_predict = predict(temp_batch, model, tokenizer, batch_size = len(temp_batch), cuda = hparams.device)
    final_predict = [value if len(temp_batch[index]) > 0 else 0.5 for index, value in enumerate(temp_predict)]
    # fluency
    n_gram = [n_gram_entropy(temp_batch[t*5:(t+1)*5]) for t in range(2)]  #n_gram_entropy() return float value

    for i, eval in enumerate(["DDSC", "DINM"]):
        for j, metric_name in enumerate(detoxify_metric):
            detoxify_performance[eval][metric_name] = {
                "response": item[eval][metric_name],
                "label": label_name[str(final_predict[i*5+j])]
            }
        detoxify_performance[eval]["fluency"] = n_gram[i]

    item_evaluate={
                "case_id": item["case_id"],
                "requested_rewrite": item["requested_rewrite"],
                "DDSC_LLM": detoxify_performance["DDSC"],
                "DINM_LLM": detoxify_performance["DINM"],
                "time": item["time"]
                    }
    return item_evaluate, final_predict + n_gram



def test_DINM(edit_data_all, editor, hparams, safety_classifier_model, safety_classifier_tokenizer, detoxify_metric, performance_dir,output_dir,counter,generate_kwargs):
    overall_performance = np.empty((0, len(detoxify_metric) * 2 + 2))  # 初始化空的 numpy 数组，+2 是为了包含 n_gram
    #####PROMPT2
    # for data in tqdm(edit_data_all):
    #     edit_data = [data,]
    #     case_id = [edit_data_['case_id'] for edit_data_ in edit_data]
    #     prompts = [edit_data_['prompt'] for edit_data_ in edit_data]
    #     # prompts_with_systemPrompt = [hparams.prefix_system_prompt + ' '+edit_data_['prompt']+' '+hparams.assistant_system_prompt+' ' +edit_data_['target_new']+hparams.prefix_system_prompt + ' '+edit_data_['prompt'] + ' ' + hparams.suffix_system_prompt for edit_data_ in edit_data]
    #     prompts_with_systemPrompt = [hparams.prefix_system_prompt + ' '+edit_data_['prompt'] + ' ' + hparams.suffix_system_prompt for edit_data_ in edit_data]
    #     target_new = [edit_data_['target_new'] for edit_data_ in edit_data]
    #     ground_truth = [edit_data_['ground_truth'] for edit_data_ in edit_data]
    #     locality_prompts = [edit_data_['locality_prompt'] for edit_data_ in edit_data]
    #     locality_prompts_with_systemPrompt = [hparams.prefix_system_prompt + ' '+edit_data_['locality_prompt'] + ' ' + hparams.suffix_system_prompt for edit_data_ in edit_data]
    #     locality_ans = [edit_data_['locality_ground_truth'] for edit_data_ in edit_data]
    #     general_prompt = [edit_data_ for edit_data_ in edit_data[0]['general_prompt']]
    #     general_prompt = [general_prompt,]
    #     general_prompt_with_systemPrompt = [hparams.prefix_system_prompt + ' '+edit_data_+ ' ' + hparams.suffix_system_prompt for edit_data_ in edit_data[0]['general_prompt']]
    #     general_prompt_with_systemPrompt = [general_prompt_with_systemPrompt,]
    #     locality_inputs = {
    #         'general knowledge constraint': {
    #             'prompt': locality_prompts,
    #             'ground_truth': locality_ans
    #         },
    #     }
    #     locality_inputs_with_systemPrompt = {
    #         'general knowledge constraint': {
    #             'prompt': locality_prompts_with_systemPrompt,
    #             'ground_truth': locality_ans
    #         },
    #     }
    
    ###PROMPT1
    for data in tqdm(edit_data_all):
        edit_data = [data,]
        case_id = [edit_data_['case_id'] for edit_data_ in edit_data]
        prompts = [edit_data_['prompt'] for edit_data_ in edit_data]
        prompts_with_systemPrompt = [edit_data_['prompt'] + ' ' + hparams.suffix_system_prompt for edit_data_ in edit_data]
        target_new = [edit_data_['target_new'] for edit_data_ in edit_data]
        ground_truth = [edit_data_['ground_truth'] for edit_data_ in edit_data]
        locality_prompts = [edit_data_['locality_prompt'] for edit_data_ in edit_data]
        locality_prompts_with_systemPrompt = [edit_data_['locality_prompt'] + ' ' + hparams.suffix_system_prompt for edit_data_ in edit_data]
        locality_ans = [edit_data_['locality_ground_truth'] for edit_data_ in edit_data]
        general_prompt = [edit_data_ for edit_data_ in edit_data[0]['general_prompt']]
        general_prompt = [general_prompt,]
        general_prompt_with_systemPrompt = [edit_data_+ ' ' + hparams.suffix_system_prompt for edit_data_ in edit_data[0]['general_prompt']]
        general_prompt_with_systemPrompt = [general_prompt_with_systemPrompt,]
        locality_inputs = {
            'general knowledge constraint': {
                'prompt': locality_prompts,
                'ground_truth': locality_ans
            },
        }
        locality_inputs_with_systemPrompt = {
            'general knowledge constraint': {
                'prompt': locality_prompts_with_systemPrompt,
                'ground_truth': locality_ans
            },
        }
        metrics, edited_model, _ ,time_DDSC,time_DINM= editor.edit(
            case_id = case_id,
            prompts=prompts,
            prompts_with_systemPrompt = prompts_with_systemPrompt,
            target_new=target_new,
            generate_kwargs = generate_kwargs,
            ground_truth=ground_truth,
            locality_inputs=locality_inputs,
            locality_inputs_with_systemPrompt = locality_inputs_with_systemPrompt,
            general_prompt = general_prompt,
            general_prompt_with_systemPrompt = general_prompt_with_systemPrompt,
            keep_original_weight=True


        )
        all_run_times_DINM.extend(time_DINM)
        all_run_times_DDSC.extend(time_DDSC)

        for item in metrics:
            # item_evaluate,  evaluate_value = evaluate_safety(item, safety_classifier_model, safety_classifier_tokenizer, detoxify_metric, cuda = hparams.device)
            item_evaluate,  evaluate_value = evaluate_safety(item, safety_classifier_model, safety_classifier_tokenizer, detoxify_metric,cuda = hparams.device)

            write_json(f'{output_dir}', item_evaluate, case_id = case_id, data_all = len(edit_data_all))
            evaluate_value = np.array(evaluate_value).reshape(1, -1)
            overall_performance = np.vstack((overall_performance, evaluate_value))
            counter += 1
            if counter % 50 == 0:
                # mean of each metric 
            
                overall_performance = np.array(overall_performance)
                metric_means = np.mean(overall_performance, axis=0)
                for i, eval in enumerate(["DDSC", "DINM"]):
                    for j, metric_name in enumerate(detoxify_metric):
                        overall_performance_avg[eval][metric_name] = 100- 100*metric_means[i*5+j]
                overall_performance_avg["DDSC"]["fluency"] = metric_means[-2]
                overall_performance_avg["DDSC"]["fluency"] = metric_means[-1]
                write_json(f'{performance_dir}/DDSCvsORI_performance_avg_{counter}.json', overall_performance_avg)
                # print(overall_performance_avg)
                # print(f"Processed {counter + 1} items (final batch), results written to file.")

    return overall_performance,all_run_times_DINM,all_run_times_DDSC


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--edited_model', required=True, type=str) ## vanilla LLM
    parser.add_argument('--editing_method', required=True, type=str)  

    parser.add_argument('--hparams_dir', required=True, type=str)  
    parser.add_argument('--safety_classifier_dir', required=True, type=str) 
    parser.add_argument('--data_dir', default='/mnt/sdb/zjk/EasyEdit_DDSC/data', type=str)
    parser.add_argument('--metrics_save_dir', default='./safety_results_10', type=str)
    parser.add_argument("--early-exit-layers", type=str, default="-1")
    parser.add_argument("--repetition_penalty", type=float, default=None)
    parser.add_argument("--relative_top", type=float, default=0.1)
    parser.add_argument("--do_sample", action="store_true")
    parser.add_argument("--num-gpus", type=int, default=1)
    parser.add_argument("--max_gpu_memory", type=int, default=27)

    args = parser.parse_args()
    
    
    
    early_exit_layers = [int(x) for x in args.early_exit_layers.split(',')]

    print(f"MODE: DDSC decoding with mature layer: {early_exit_layers[-1]} and premature layers: {early_exit_layers[:-1]}")
    mode = "DDSC"
    end_layer = early_exit_layers[-1]
    safe_layer = None
    toxic_layer = None
    candidate_layers = early_exit_layers[:-1]
    safe_layer_dist = {l:0 for l in candidate_layers}
    if args.repetition_penalty is None:
        args.repetition_penalty = 1.7   #repetition_penalty default=1.7
    num_gpus = args.num_gpus

    generate_kwargs = dict(do_sample=False,  max_new_tokens=256,temperature=0.7, repetition_penalty=args.repetition_penalty, mode=mode, end_layer=end_layer, safe_layer=safe_layer, candidate_layers=candidate_layers, remove_stop_words=True, relative_top=args.relative_top)
    editing_hparams = DINMHyperParams
    output_dir = f'{args.metrics_save_dir}/DDSC vs ori.json'
    performance_dir = f'{args.metrics_save_dir}'
    #### some variables used for statistical results 
    if not os.path.exists(args.metrics_save_dir):
        os.mkdir(args.metrics_save_dir)
    print(f"Results will be stored at {output_dir}")
    overall_performance_avg = {
        "DDSC": {},
        "DINM": {}
    }
    
    detoxify_metric = ["DS", "DG_onlyQ", "DG_otherA", "DG_otherQ", "DG_otherAQ"]
    #dataset
    edit_data_all = SafetyDataset(f'{args.data_dir}/SafeEdit_test_ori.json')
    hparams = editing_hparams.from_hparams(args.hparams_dir)

    # classifier
    safety_classifier_model = RobertaForSequenceClassification.from_pretrained(args.safety_classifier_dir).to(f"cuda:{hparams.device}") 
    safety_classifier_tokenizer = RobertaTokenizer.from_pretrained(args.safety_classifier_dir)

    editor = SafetyEditor.from_hparams(hparams)
    counter = 0
    # edit_data_all = edit_data_all[0:2]
    if args.editing_method == "DINM":
        overall_performance,all_run_times_DINM,all_run_times_DDSC = test_DINM(edit_data_all, editor, hparams, safety_classifier_model, safety_classifier_tokenizer, detoxify_metric, performance_dir,output_dir,counter,generate_kwargs)
    else:
        print("This method is currently not supported")
        
    file_name = '/mnt/sdb/zjk/EasyEdit_DDSC/LLAMA/test/all_run_times_DINM.json'
    with open(file_name, 'w') as file:
        json.dump(all_run_times_DINM, file, indent=4)

    print(f"所有生成的 run_times_DINM 已保存到 {file_name}")
    file_name = '/mnt/sdb/zjk/EasyEdit_DDSC/LLAMA/test/all_run_times_DDSC.json'
    with open(file_name, 'w') as file:
        json.dump(all_run_times_DDSC, file, indent=4)

    print(f"所有生成的 run_times_DDSC 已保存到 {file_name}")
    # mean of each metric 
    overall_performance = np.array(overall_performance)
    metric_means = np.mean(overall_performance, axis=0)
    for i, eval in enumerate(["DDSC", "DINM"]):
        for j, metric_name in enumerate(detoxify_metric):
            overall_performance_avg[eval][metric_name] = 100- 100*metric_means[i*5+j]
    overall_performance_avg["DDSC"]["fluency"] = metric_means[-2]
    overall_performance_avg["DDSC"]["fluency"] = metric_means[-1]
    write_json(f'{args.metrics_save_dir}/DDSC_NEW vs Ori_performance_avg.json', overall_performance_avg)
    print(overall_performance_avg)
    print(f'{args.editing_method}_{args.edited_model} is done ')





# DDSC edits mistral-7b
# python run_safety_editing.py --editing_method=DINM --edited_model=mistral-7b --hparams_dir=./hparams/DINM/mistral-7b --safety_classifier_dir=zjunlp/SafeEdit-Safety-Classifier --metrics_save_dir=./safety_results

# DDSC edits llama-2-7b-chat
# python run_safety_editing.py --editing_method=DINM --edited_model=llama-2-7b-chat --hparams_dir=./hparams/DINM/llama-7b --safety_classifier_dir=zjunlp/SafeEdit-Safety-Classifier --metrics_save_dir=./safety_results
    






    
